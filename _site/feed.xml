<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>TX Blog</title>
    <description>Every failure is leading towards success.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sun, 09 Feb 2025 18:21:49 +0000</pubDate>
    <lastBuildDate>Sun, 09 Feb 2025 18:21:49 +0000</lastBuildDate>
    <generator>Jekyll v4.3.4</generator>
    
      <item>
        <title>DeepSeek</title>
        <description>&lt;h2 id=&quot;v3-base&quot;&gt;V3-Base&lt;/h2&gt;
&lt;p&gt;671B decoder-only transformer
Self-supervised pre-training:
No label, train on first half of sentence to predict the second half&lt;/p&gt;

&lt;h2 id=&quot;r1-zero&quot;&gt;R1-Zero&lt;/h2&gt;

&lt;p&gt;Trained on v3-base using reinforcement learning:
Supervised fine tuning (SFT)
Reinforcement Learning from Human Feedback (RLHF)&lt;/p&gt;

&lt;h2 id=&quot;r1&quot;&gt;R1&lt;/h2&gt;

&lt;p&gt;Use R1-zero to come up with good CoT
Use those CoT to fine-tune V3-Base
Then train with SFT to get R1&lt;/p&gt;

&lt;h2 id=&quot;llamaqwen-distilled&quot;&gt;Llama/Qwen Distilled&lt;/h2&gt;

&lt;p&gt;Use R1 to come up with 800k samples, then use those
to train other open-source models: Meta &amp;amp; Qwen&lt;/p&gt;

&lt;h2 id=&quot;other-techniques&quot;&gt;Other techniques&lt;/h2&gt;

&lt;p&gt;MoE system
Multihead latent Attention (MLA)
Distilling using O1
Low-level programming GPU cores
Group Relative Policy Optimization (GRPO)&lt;/p&gt;

&lt;h2 id=&quot;some-math&quot;&gt;SOme Math&lt;/h2&gt;
&lt;p&gt;\(\frac{1}{2}\)&lt;/p&gt;

&lt;p&gt;Euler’s formula: $e^{i\pi} + 1 = 0$&lt;/p&gt;

\[\sum_{n=1}^{\infty} \frac{1}{n^2} = \frac{\pi^2}{6}\]
</description>
        <pubDate>Sat, 08 Feb 2025 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/2025/02/08/DeepSeek/</link>
        <guid isPermaLink="true">http://localhost:4000/2025/02/08/DeepSeek/</guid>
        
        <category>Deep learning</category>
        
        <category>LLM</category>
        
        
      </item>
    
      <item>
        <title>Data Science Internship</title>
        <description>&lt;h2 id=&quot;data-science-intern-at-natwest-group&quot;&gt;Data Science Intern at Natwest Group&lt;/h2&gt;
&lt;p&gt;Jun - Aug 2024&lt;/p&gt;

&lt;h3 id=&quot;overview&quot;&gt;Overview&lt;/h3&gt;

&lt;p&gt;I was placed into the Data Science team within Natwest Group Solutions, the part of Natwest that focuses on coming up with digital solutions for other parts of the bank. In particular, I was in charge of improving the machine learning classifier for the Suspicious Activity Reports (SAR) which the FinCrime team need to analyze.&lt;/p&gt;

&lt;h3 id=&quot;challenges&quot;&gt;Challenges&lt;/h3&gt;

&lt;p&gt;The SAR are written by different humans, without a set strict format in general. The purpose is to classify each SAR into a category (e.g. Money laundering, gambling, fraud etc), for further analysis. The old classifier used by the FinCrime team is a rule-based model (based on a bunch of if-elses), and can lead to low classication accuracy especially in terms of minority groups. A major challenge for using a machine learning model that can be trained for higher accuracy is the lack of labeled data. While the bank has several thousand SARs per month, human labor are needed to label each SAR with the right category, and we had only 50ish labeled SARs per month.&lt;/p&gt;

&lt;h3 id=&quot;my-approach&quot;&gt;My Approach&lt;/h3&gt;
&lt;p&gt;I developed a novel approach by using LLM to generate synthetic SARs for each category. This way we can scale up the data and allow the training of ML classifiers. To ensure the quality, we employed clustering analysis by looking at the vector embeddings of the original data and the generated data, both fed into the embedding layer of an LLM into a (1546,) vector. We used PCA to find the two most important dimension for plotting the data vectors, computed average consine similarity for both old and new data, and also constructed a cosine-distance based KNN graph. All evidence suggested there isn’t a systematic difference between the old and the new synthetic text data.&lt;/p&gt;

&lt;p&gt;Hence we were able to train a random forest model on top of all the data and achieved a big increase in average classication accuracy across all categories.&lt;/p&gt;

&lt;p&gt;An alternative approach where we directly prompt the LLM to do the classication also worked, though it still suffers from hallucinations and occasionally create categories that didn’t exist.&lt;/p&gt;

&lt;h3 id=&quot;technologies-used&quot;&gt;Technologies Used&lt;/h3&gt;
&lt;p&gt;We used OpenAI API through Azure.&lt;/p&gt;

&lt;p&gt;The coding environment was based on the AWS Sagemaker Studio, and data are stored in the AWS S3 database.&lt;/p&gt;

</description>
        <pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/2024/11/01/Natwest/</link>
        <guid isPermaLink="true">http://localhost:4000/2024/11/01/Natwest/</guid>
        
        <category>python</category>
        
        <category>AWS</category>
        
        <category>Internship</category>
        
        
      </item>
    
      <item>
        <title>Machine Learning Internship</title>
        <description>&lt;h2 id=&quot;ml-internship-at-imperials-ai-lab-deepwok&quot;&gt;ML Internship at Imperial’s AI Lab &lt;a href=&quot;https://deepwok.github.io/&quot;&gt;Deepwok&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Apr - Sep 2024&lt;/p&gt;

&lt;h3 id=&quot;project-overview&quot;&gt;Project Overview&lt;/h3&gt;
&lt;p&gt;DNA language models apply techniques from natural language processing (NLP) to model biological sequences, treating DNA as a “language” to capture patterns, structures, and functional elements in genomic data. These models aim to understand and predict biological phenomena by encoding DNA sequences in a format that machine learning models can use.&lt;/p&gt;

&lt;p&gt;Tokenization Methods: In DNA language models, tokenization is the process of breaking down DNA sequences into manageable units, or “tokens,” which represent the genetic information. State-of-the-art DNA models like Hyena used a mere character-level naive tokenizer, simply translating ‘A’, ‘C’, ‘G’, ‘T’ to fixed integers. We were experimenting the adoptation of the 
Byte-Pair Encoding (BPE) method used in GPT3 and GPT4. The idea is to combine frequently occurring character pairs into single tokens iteratively, creating a hierarchy of tokens that balances between single characters and multi-character sequences.&lt;/p&gt;

&lt;h3 id=&quot;contribution&quot;&gt;Contribution&lt;/h3&gt;
&lt;p&gt;As part of the team to develop a BPE-based DNA LLM, my work mostly involves two parts.&lt;/p&gt;

&lt;p&gt;Firstly, I worked on fine-tuning the hyperparameters like the vocab size for BPE, sequence length, context window etc for the DNA LLM. We built our model based on Olmo LLM &lt;a href=&quot;https://huggingface.co/allenai/OLMo-1B&quot;&gt;1B&lt;/a&gt; and &lt;a href=&quot;https://huggingface.co/allenai/OLMo-7B&quot;&gt;7B&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Secondly, I mostly worked on developping a modular evaluation framework that integrates our DNA LLM with downstream tasks (e.g. gene prediction, Protein-DNA interaction prediction) and benchmarks e.g. &lt;a href=&quot;https://github.com/ML-Bioinfo-CEITEC/genomic_benchmarks&quot;&gt;Genomic Benchmarks&lt;/a&gt;, enabling automated performance evaluations and comparisons.&lt;/p&gt;

&lt;p&gt;For example, this involves comparing our model’s performance with &lt;a href=&quot;https://github.com/MAGICS-LAB/DNABERT_2&quot;&gt;DNABERT2&lt;/a&gt; and &lt;a href=&quot;https://github.com/HazyResearch/hyena-dna&quot;&gt;Hyena&lt;/a&gt;, both were established State-of-the-Art models on Genomic Benchmarks.&lt;/p&gt;
</description>
        <pubDate>Sun, 01 Sep 2024 00:00:00 +0100</pubDate>
        <link>http://localhost:4000/2024/09/01/Deepwok-Lab-en/</link>
        <guid isPermaLink="true">http://localhost:4000/2024/09/01/Deepwok-Lab-en/</guid>
        
        <category>Pytorch</category>
        
        <category>LLM</category>
        
        <category>Machine Learning</category>
        
        <category>Internship</category>
        
        
      </item>
    
      <item>
        <title>Optiver &amp; Imperial Trading Academy</title>
        <description>&lt;h2 id=&quot;optiver--imperial-trading-academy&quot;&gt;Optiver &amp;amp; Imperial Trading Academy&lt;/h2&gt;
&lt;p&gt;Nov 7 - 29, 2023&lt;/p&gt;

&lt;h3 id=&quot;overview&quot;&gt;Overview&lt;/h3&gt;
&lt;p&gt;This is an 8-session &lt;a href=&quot;https://optiver.com/recruitment-events/imperial-college-london-x-optiver-trading-academy/&quot;&gt;project&lt;/a&gt; that covers both the foundations of options theory and hands-on algorithm development and testing. At the end of the session, I participated with other student traders together in a real-time trading challenge by designing our own Python trading algorithm based on real financial market data.&lt;/p&gt;

&lt;h3 id=&quot;what-i-did&quot;&gt;What I did&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Developed a delta-hedging trading algorithm that autonomously traded options in Optiver’s simulated exchange, Optibook, leveraging the Black-Scholes model for option pricing.&lt;/li&gt;
  &lt;li&gt;Trained an SVM model to dynamically adjust the credit for bid/ask prices based on liquidity, volatility, time to expiry, and historical price movements.&lt;/li&gt;
  &lt;li&gt;Achieved positive PnL of $500,000, ranked in top 20% of participants in the contest with over 50 student traders.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 01 Nov 2023 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/2023/11/01/optiver/</link>
        <guid isPermaLink="true">http://localhost:4000/2023/11/01/optiver/</guid>
        
        <category>volatility trading</category>
        
        <category>equity option pricing</category>
        
        <category>quantitative research</category>
        
        
      </item>
    
  </channel>
</rss>
