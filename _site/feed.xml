<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>TX Blog</title>
    <description>Every failure is leading towards success.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Tue, 05 Nov 2024 19:07:44 +0000</pubDate>
    <lastBuildDate>Tue, 05 Nov 2024 19:07:44 +0000</lastBuildDate>
    <generator>Jekyll v4.3.4</generator>
    
      <item>
        <title>Data Science Internship</title>
        <description>&lt;h3 id=&quot;timeline&quot;&gt;TimeLine&lt;/h3&gt;
&lt;p&gt;Jun - Aug 2024&lt;/p&gt;

&lt;h3 id=&quot;overview&quot;&gt;Overview&lt;/h3&gt;

&lt;p&gt;I was placed into the Data Science team within Natwest Group Solutions, the part of Natwest that focuses on coming up with digital solutions for other parts of the bank. In particular, I was in charge of improving the machine learning classifier for the Suspicious Activity Reports (SAR) which the FinCrime team need to analyze.&lt;/p&gt;

&lt;h3 id=&quot;challenges&quot;&gt;Challenges&lt;/h3&gt;

&lt;p&gt;The SAR are written by different humans, without a set strict format in general. The purpose is to classify each SAR into a category (e.g. Money laundering, gambling, fraud etc), for further analysis.The old classifier used by the FinCrime team is a rule-based model (based on a bunch of if-elsees), and can lead to low classication accuracy especially in terms of minority groups. A major challenge for using a machine learning model that can be trained for higher accuracy is the lack of labeled data. While the bank has several thousand SARs per month, human labor are needed to label each SAR with the right category, and we had only 50ish labeled SARs per month.&lt;/p&gt;

&lt;h3 id=&quot;my-approach&quot;&gt;My Approach&lt;/h3&gt;
&lt;p&gt;I developed a novel approach by using LLM to generate synthetic SAR for each category. This way we can scale up the data and allow the training of ML classifiers. To ensure the quality, we employed clustering analysis by looking at the vector embeddings of the original data and the generated data, both fed into the embedding layer of an LLM into a (1546,) vector. We used PCA to find the two most important dimension for plotting the data vectors, computed average consine similarity for both old and new data, and also constructed a cosine-distance based KNN graph. All evidence suggested there isn’t a systematic difference between the old and the new synthetic text data.&lt;/p&gt;

&lt;p&gt;Hence we were able to train a random forest model on top of all the data and achieved a big increase in average classication accuracy across all categories.&lt;/p&gt;

&lt;p&gt;An alternative approach where we directly prompt the LLM to do the classication also worked, though it still suffers from hallucinations and occasionally create categories that didn’t exist.&lt;/p&gt;

&lt;h3 id=&quot;technologies-used&quot;&gt;Technologies Used&lt;/h3&gt;
&lt;p&gt;We used OpenAI API through Azure.&lt;/p&gt;

&lt;p&gt;The coding environment was based on the AWS Sagemaker Studio, and data are stored in the AWS S3 database.&lt;/p&gt;

</description>
        <pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/2024/11/01/Natwest/</link>
        <guid isPermaLink="true">http://localhost:4000/2024/11/01/Natwest/</guid>
        
        <category>python</category>
        
        <category>AWS</category>
        
        <category>Internship</category>
        
        
      </item>
    
      <item>
        <title>Machine Learning Internship</title>
        <description>&lt;h3 id=&quot;timeline&quot;&gt;TimeLine&lt;/h3&gt;
&lt;p&gt;Apr - Sep 2024&lt;/p&gt;

&lt;h3 id=&quot;project-overview&quot;&gt;Project Overview&lt;/h3&gt;
&lt;p&gt;DNA language models apply techniques from natural language processing (NLP) to model biological sequences, treating DNA as a “language” to capture patterns, structures, and functional elements in genomic data. These models aim to understand and predict biological phenomena by encoding DNA sequences in a format that machine learning models can use.&lt;/p&gt;

&lt;p&gt;Tokenization Methods: In DNA language models, tokenization is the process of breaking down DNA sequences into manageable units, or “tokens,” which represent the genetic information. State-of-the-art DNA models like Hyena used a mere character-level naive tokenizer, simply translating ‘A’, ‘C’, ‘G’, ‘T’ to fixed integers. We were experimenting the adoptation of the 
Byte-Pair Encoding (BPE) method used in GPT3 and GPT4. The idea is to combine frequently occurring character pairs into single tokens iteratively, creating a hierarchy of tokens that balances between single characters and multi-character sequences.&lt;/p&gt;

&lt;h3 id=&quot;contribution&quot;&gt;Contribution&lt;/h3&gt;

&lt;p&gt;Most of my work is software development on &lt;a href=&quot;https://github.com/jianyicheng/mase-tools&quot;&gt;MASE&lt;/a&gt;, or more specific, on Machop, MASE’s software stack.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/jianyicheng/mase-tools&quot;&gt;Mase (Machine-Learning Accelerator System Exploration Tools)&lt;/a&gt;&lt;/strong&gt; , provides an efficient and scalable approach for exploring accelerator systems to compute large ML models by directly mapping onto an efficient streaming accelerator system.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Mase provide Command Line Interface to train a model from scratch using specified dataset. To estimate efficiency of new dataflow accelerator, I integrated RepVGG models support into Mase, organized and documented detailed procedures for future developers.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;2.Exploration of the DNN hardware accelerator required hardware-software co-simulation.  I helped with scripts writing that ship metadata to the hardware team, which would helps verify the integrity of the hardware implementation and could help identify discrepancies. Scripts include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Transform Script:&lt;/strong&gt; Run a transformation pass on the specified model, e.g. quantization or pruning.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Search Script:&lt;/strong&gt; Apply a given search strategy over a search space to optimize a specified set of hardware/software metrics.&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Sun, 01 Sep 2024 00:00:00 +0100</pubDate>
        <link>http://localhost:4000/2024/09/01/Deepwok-Lab-en/</link>
        <guid isPermaLink="true">http://localhost:4000/2024/09/01/Deepwok-Lab-en/</guid>
        
        <category>Pytorch</category>
        
        <category>LLM</category>
        
        <category>Machine Learning</category>
        
        <category>Internship</category>
        
        
      </item>
    
      <item>
        <title>Optiver &amp; Imperial Trading Academy</title>
        <description>&lt;h3 id=&quot;timeline&quot;&gt;TimeLine&lt;/h3&gt;
&lt;p&gt;Nov 2023&lt;/p&gt;

&lt;h3 id=&quot;overview&quot;&gt;Overview&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Developed a delta-hedging trading algorithm that autonomously traded options in Optiver’s simulated exchange, Optibook, leveraging the Black-Scholes model for option pricing.&lt;/li&gt;
  &lt;li&gt;Trained an SVM model to dynamically adjust the credit for bid/ask prices based on liquidity, volatility, time to expiry, and historical price movements.&lt;/li&gt;
  &lt;li&gt;Achieved positive PnL of $500,000, ranked in top 20% of participants in the contest with over 50 student traders.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 01 Nov 2023 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/2023/11/01/optiver/</link>
        <guid isPermaLink="true">http://localhost:4000/2023/11/01/optiver/</guid>
        
        <category>volatility trading</category>
        
        <category>equity option pricing</category>
        
        <category>quantitative research</category>
        
        
      </item>
    
  </channel>
</rss>
